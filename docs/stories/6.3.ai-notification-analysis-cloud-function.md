# Story 6.3: AI Notification Analysis Cloud Function

## Status

**Draft**

---

## Story

**As a** user,
**I want** AI to intelligently decide which conversations deserve notifications,
**so that** I only get notified about truly important messages, not general chatter.

---

## Acceptance Criteria

**Cloud Function: analyzeForNotification**
1. Function signature: `analyzeForNotification(conversationId: String, userId: String) -> NotificationDecision`
2. Returns: `{ shouldNotify: Bool, reason: String, notificationText: String, priority: "high" | "medium" | "low" }`
3. Authentication: Validates Firebase Auth token, verifies user is conversation participant
4. Input validation: Checks conversationId exists, userId valid, conversation has unread messages

**AI Analysis Logic:**
5. Step 1: Fetch recent conversation messages (last 30 messages or 15 minutes, whichever is fewer)
6. Step 2: Filter to unread messages for this user (don't analyze already-read content)
7. Step 3: Retrieve user context via RAG (Story 6.2 dependencies)
8. Step 4: Call OpenAI GPT-4 with structured prompt (see below)
9. Step 5: Parse JSON response, validate structure

**LLM Prompt Structure:**
10. System prompt includes:
    - Role: "Notification assistant for remote team professionals"
    - Notification criteria: Mentions, urgent requests, direct questions, decisions affecting user, blockers, production issues
    - Non-notification criteria: General chat, FYI updates user isn't involved in, social/casual conversation
    - Output format: JSON with shouldNotify, reason, notificationText, priority
11. User context included: User's recent activity, preferences, active hours, priority keywords
12. Conversation context: Last N messages formatted with sender names, timestamps, message text
13. Temperature: 0.3 (more deterministic, consistent decisions)

**Notification Decision Quality:**
14. **Must notify:** Direct @mentions of user (100% notification rate)
15. **Must notify:** Direct questions to user ("Can you...", "Could you...", "Would you...")
16. **Must notify:** Decisions affecting user's work (detected via context: project names, user responsibilities)
17. **Should notify:** Urgent/time-sensitive keywords in user's priority list (default: "urgent", "ASAP", "production down", "blocker")
18. **Should not notify:** General team chat not involving user
19. **Should not notify:** Social/casual messages ("lol", "thanks!", emoji reactions)

**Notification Text Generation:**
20. Notification text clear and actionable: "Sarah mentioned you: 'Can you review the API design?'"
21. Notification text includes sender and key context
22. Notification text max 100 characters (iOS/Android limits)
23. Priority level affects iOS notification presentation (high = banner, medium = notification center, low = badge only)

**Caching & Cost Optimization:**
24. Check cache before LLM call: cacheKey = `notification_${conversationId}_${unreadMessagesHash}`
    - Where unreadMessagesHash = SHA256 hash of unread message IDs for this user (sorted)
    - Cache only invalidates when NEW unread messages arrive (not all messages)
25. Cache hit: Return cached decision (< 1 second)
26. Cache miss: Call LLM, cache result with 1-hour expiration
27. Cache TTL: Cache expires after 1 hour OR when new unread messages arrive, whichever comes first

**Error Handling:**
28. OpenAI rate limit (429): Return `{ shouldNotify: false, reason: "AI temporarily unavailable" }`
29. Timeout (>10 seconds): Return fallback decision based on simple heuristics (mentions = notify)
30. Network error: Log error, return `{ shouldNotify: false }`
31. Invalid response from LLM: Retry once, then fallback to heuristics

**Fallback Heuristics (when AI unavailable):**
32. If message contains "@username" where username = current user â†’ notify (high priority)
33. If message contains priority keywords â†’ notify (medium priority)
34. If message is direct question (ends with "?") and mentions user â†’ notify (medium priority)
35. Otherwise â†’ don't notify

**Testing:**
36. Unit test: Mock OpenAI responses with recorded JSON fixtures. Test prompt structure and context formatting, NOT LLM behavior.
37. Unit test: JSON parsing handles all expected response formats (valid, missing fields, malformed)
38. Integration test: Use recorded LLM responses (fixtures). Verify client correctly parses and applies decision.
39. Integration test: Test fallback heuristics directly (bypass LLM). Verify direct mention triggers notify.
40. Integration test: Test fallback heuristics directly. Verify general chat triggers no notify.
41. Integration test: Cache hit returns instantly (< 1 second) - use real cache, mock LLM
42. Integration test: AI unavailable (mock 429 error) falls back to heuristics
43. Performance test: Analysis completes within 8 seconds (including RAG)
44. Manual validation: Test with 10 sample conversations, manually verify AI decisions make sense

---

## Tasks / Subtasks

- [ ] Create analyzeForNotification Cloud Function (AC: 1-4)
  - [ ] Create `functions/src/analyzeForNotification.ts`
  - [ ] Export as `onCall` function with authentication check
  - [ ] Validate input parameters: conversationId, userId
  - [ ] Verify user is participant in conversation (Firestore query)
  - [ ] Return error if user not authorized

- [ ] Implement conversation data fetching (AC: 5-6)
  - [ ] Query Firestore for recent messages (last 30 or 15 minutes)
  - [ ] Filter to unread messages: `!message.readBy.includes(userId)`
  - [ ] If no unread messages, return `{ shouldNotify: false, reason: "No unread messages" }`
  - [ ] Format messages for LLM context

- [ ] Integrate RAG system (AC: 7, Story 6.2 dependency)
  - [ ] Import `getUserRecentContext` from Story 6.2
  - [ ] Import `findRelevantMessages` from Story 6.2
  - [ ] Fetch user's recent activity (last 100 messages, 7 days)
  - [ ] Perform semantic search for related past conversations
  - [ ] Load user notification preferences

- [ ] Design LLM prompt (AC: 10-13)
  - [ ] Create `functions/src/prompts/notification-analysis-prompt.ts`
  - [ ] Define system prompt template
  - [ ] Define user context formatting
  - [ ] Define conversation formatting
  - [ ] Include notification criteria and examples
  - [ ] Specify JSON output schema

- [ ] Implement OpenAI GPT-4 call (AC: 8-9)
  - [ ] Use `openai.chat.completions.create()` with GPT-4 model
  - [ ] Set temperature: 0.3
  - [ ] Set response_format: `{ type: "json_object" }`
  - [ ] Parse JSON response
  - [ ] Validate response structure matches NotificationDecision schema

- [ ] Implement notification decision logic (AC: 14-19)
  - [ ] Validate LLM identifies direct @mentions (must notify)
  - [ ] Validate LLM identifies direct questions (must notify)
  - [ ] Validate LLM identifies urgent keywords (should notify)
  - [ ] Validate LLM suppresses general chat (should not notify)
  - [ ] Test with 10 sample conversations covering all criteria

- [ ] Implement notification text generation (AC: 20-23)
  - [ ] Extract sender name from message
  - [ ] Format notification text: "{Sender}: {message snippet}"
  - [ ] Truncate to 100 characters max
  - [ ] Set priority level based on LLM decision

- [ ] Implement caching layer (AC: 24-27)
  - [ ] Create Firestore collection: `ai_notification_cache`
  - [ ] Generate cache key: hash of conversationId + latestMessageId
  - [ ] Check cache before calling LLM
  - [ ] Store result in cache with 1-hour TTL
  - [ ] Invalidate cache on new messages (Firestore trigger)

- [ ] Implement error handling (AC: 28-31)
  - [ ] Catch OpenAI rate limit errors (status 429)
  - [ ] Implement timeout after 10 seconds
  - [ ] Retry logic: Retry once on network error
  - [ ] Log all errors to Firebase Logging

- [ ] Implement fallback heuristics (AC: 32-35)
  - [ ] Create `functions/src/helpers/fallback-notification-logic.ts`
  - [ ] Detect @mentions using regex
  - [ ] Detect priority keywords in message text
  - [ ] Detect direct questions (ends with "?")
  - [ ] Return simple decision based on heuristics

- [ ] Write unit tests (AC: 36-37)
  - [ ] Test: Prompt includes user context correctly
  - [ ] Test: JSON response parsing handles all formats
  - [ ] Test: Fallback heuristics detect mentions correctly
  - [ ] Test: Fallback heuristics detect questions correctly
  - [ ] Test: Cache key generation is consistent

- [ ] Write integration tests (AC: 38-42)
  - [ ] Test: Direct mention returns shouldNotify=true
  - [ ] Test: General chat returns shouldNotify=false
  - [ ] Test: Urgent keyword returns shouldNotify=true with medium priority
  - [ ] Test: Cache hit returns < 1 second
  - [ ] Test: AI unavailable falls back to heuristics
  - [ ] Test: User not in conversation returns error

- [ ] Performance testing (AC: 43)
  - [ ] Measure end-to-end latency (target: < 8 seconds)
  - [ ] Measure cache hit latency (target: < 1 second)
  - [ ] Measure RAG overhead (Story 6.2: < 3 seconds)
  - [ ] Measure LLM call latency (typical: 3-5 seconds)
  - [ ] Optimize if performance targets not met

---

## Dev Notes

### Architecture Context

**Cloud Function Flow:**
```
analyzeForNotification (onCall)
  â†“
1. Authenticate & Validate
  â†“
2. Check Cache (ai_notification_cache)
  â†“ (cache miss)
3. Fetch Conversation Messages (Firestore)
  â†“
4. Load User Context (RAG - Story 6.2)
  â†“
5. Build LLM Prompt
  â†“
6. Call OpenAI GPT-4
  â†“
7. Parse JSON Response
  â†“
8. Store in Cache
  â†“
9. Return NotificationDecision
```

**Fallback on Error:**
```
LLM Error (rate limit, timeout, network)
  â†“
Log Error
  â†“
Apply Fallback Heuristics
  â†“
Return Simple Decision
```

### Relevant Source Tree

**New Files:**
- `functions/src/analyzeForNotification.ts` - Main Cloud Function
- `functions/src/prompts/notification-analysis-prompt.ts` - LLM prompt templates
- `functions/src/helpers/fallback-notification-logic.ts` - Simple heuristics
- `functions/test/analyzeForNotification.test.ts` - Unit tests
- `functions/test/integration/notification-analysis.test.ts` - Integration tests

**Modified Files:**
- `functions/src/index.ts` - Export analyzeForNotification
- `firestore.rules` - Add rules for ai_notification_cache collection

### Key Technical Decisions

**1. LLM Model Selection:**
Use OpenAI GPT-4 (not GPT-3.5):
- **Why:** Better context understanding, fewer hallucinations, more consistent decisions
- **Cost:** ~$0.03 per 1K tokens (input) + $0.06 per 1K tokens (output)
- **Typical request:** ~2K tokens input + 200 tokens output = ~$0.08 per analysis
- **With 60% cache hit rate:** Effective cost ~$0.03 per analysis

**2. Structured JSON Output:**
```typescript
{
  shouldNotify: boolean,
  reason: string, // AI's reasoning for decision
  notificationText: string, // User-facing notification text
  priority: "high" | "medium" | "low"
}
```

Use OpenAI's `response_format: { type: "json_object" }` to guarantee JSON output.

**3. System Prompt Template:**
```typescript
const SYSTEM_PROMPT = `You are a notification assistant for remote team professionals. Your job is to analyze conversation messages and decide if the user should be notified.

ALWAYS NOTIFY if:
- User is directly mentioned (@username or by name)
- User is asked a direct question ("Can you...", "Could you...", "Would you...")
- A decision is made that affects the user's work or responsibilities
- There's an urgent/time-sensitive request related to user's projects
- Production issue or blocker is mentioned that affects user

NEVER NOTIFY if:
- General team chat that doesn't involve the user
- FYI updates the user isn't responsible for
- Social/casual conversation (jokes, "thanks", emoji reactions)
- Information already known to user (based on user context)

User Context:
{userContext}

User Preferences:
- Active hours: {activeHours}
- Priority keywords: {priorityKeywords}

Conversation Messages:
{messages}

Respond ONLY with JSON in this format:
{
  "shouldNotify": true/false,
  "reason": "brief explanation of decision",
  "notificationText": "clear, actionable notification text (max 100 chars)",
  "priority": "high" | "medium" | "low"
}`;
```

**4. Message Formatting:**
```typescript
function formatMessagesForLLM(messages: Message[]): string {
  return messages.map(msg =>
    `[${msg.timestamp.toISOString()}] ${msg.senderName}: ${msg.text}`
  ).join('\n');
}
```

**5. Caching Strategy (Improved for Better Hit Rate):**
```typescript
import crypto from 'crypto';

// Cache key includes conversationId + hash of unread message IDs
// Only invalidates when NEW unread messages arrive (not all messages)
function generateCacheKey(conversationId: string, unreadMessageIds: string[]): string {
  // Sort for consistency
  const sortedIds = unreadMessageIds.sort();

  // Hash the unread message IDs
  const hash = crypto.createHash('sha256')
    .update(sortedIds.join(','))
    .digest('hex')
    .substring(0, 16); // First 16 chars sufficient

  return `notification_${conversationId}_${hash}`;
}

// Cache entry
{
  cacheKey: string,
  decision: NotificationDecision,
  unreadMessageIds: string[], // Store for validation
  createdAt: Timestamp,
  expiresAt: Timestamp // +1 hour
}

// Cache invalidation: Only when new unread messages arrive for this user
// Result: Higher cache hit rate (60% â†’ 75%+)
```

**6. Fallback Heuristics:**
```typescript
function fallbackNotificationDecision(
  messages: Message[],
  userId: string,
  preferences: NotificationPreferences
): NotificationDecision {
  // Check for @mention
  const hasMention = messages.some(msg =>
    msg.text.includes(`@${userId}`) || msg.text.includes(`@${userName}`)
  );
  if (hasMention) {
    return {
      shouldNotify: true,
      reason: "User mentioned (fallback heuristic)",
      notificationText: extractMentionText(messages),
      priority: "high"
    };
  }

  // Check for priority keywords
  const hasPriorityKeyword = messages.some(msg =>
    preferences.priorityKeywords.some(keyword =>
      msg.text.toLowerCase().includes(keyword.toLowerCase())
    )
  );
  if (hasPriorityKeyword) {
    return {
      shouldNotify: true,
      reason: "Priority keyword detected (fallback)",
      notificationText: extractPriorityText(messages),
      priority: "medium"
    };
  }

  // Default: Don't notify
  return {
    shouldNotify: false,
    reason: "No notification triggers found (fallback)",
    notificationText: "",
    priority: "low"
  };
}
```

**7. Performance Optimization:**
- Parallel execution: Fetch messages, user context, and cache check concurrently
- Message limit: Only analyze last 30 messages (reduces token cost + processing time)
- User context caching: 10-minute TTL (Story 6.2)
- Notification decision caching: 1-hour TTL (this story)

```typescript
// Parallel fetching
const [messages, userContext, cachedDecision] = await Promise.all([
  fetchRecentMessages(conversationId, 30),
  getUserRecentContext(userId),
  checkCache(cacheKey)
]);

if (cachedDecision && !isCacheExpired(cachedDecision)) {
  return cachedDecision.decision; // < 1 second
}
```

### Cost Estimation

**Per Analysis (No Cache):**
- RAG context retrieval: $0.0004 (Story 6.2)
- GPT-4 LLM call: ~$0.08 (2K input tokens + 200 output tokens)
- Firestore writes (cache): $0.00018
- **Total: ~$0.08 per analysis**

**With 60% Cache Hit Rate:**
- 60% cache hits: $0.0004 (just Firestore read)
- 40% cache misses: $0.08
- **Effective cost: 0.6 * $0.0004 + 0.4 * $0.08 = ~$0.032 per analysis**

**Monthly Cost (1000 users, 10 analyses/user/day):**
- 1000 users * 10 analyses/day * 30 days * $0.032 = **$9,600/month**

Note: This is a demo. Production would need:
- Lower analysis frequency (smarter triggers)
- Higher cache hit rate (>80%)
- Batch processing for multiple users
- Or use cheaper model (GPT-3.5: ~$0.003 per analysis)

### Notification Decision Examples

**Example 1: Direct Mention (Must Notify)**
```
Input:
User: alice@example.com
Messages:
  - [10:15] Bob: "Hey team, we need to finalize the API design"
  - [10:16] Sarah: "@Alice can you review the endpoints?"

Output:
{
  "shouldNotify": true,
  "reason": "User directly mentioned in conversation",
  "notificationText": "Sarah mentioned you: Can you review the endpoints?",
  "priority": "high"
}
```

**Example 2: General Chat (Should Not Notify)**
```
Input:
User: alice@example.com
Messages:
  - [10:15] Bob: "Great job on the demo!"
  - [10:16] Sarah: "Thanks! ðŸŽ‰"

Output:
{
  "shouldNotify": false,
  "reason": "Social conversation not involving user",
  "notificationText": "",
  "priority": "low"
}
```

**Example 3: Urgent Keyword (Should Notify)**
```
Input:
User: alice@example.com
Preferences: priorityKeywords = ["production down", "urgent"]
Messages:
  - [10:15] Bob: "Production down! Database connection failing"

Output:
{
  "shouldNotify": true,
  "reason": "Urgent priority keyword detected affecting user's area",
  "notificationText": "Bob: Production down! Database connection failing",
  "priority": "high"
}
```

### Testing LLM-Based Logic

**CRITICAL: DO NOT test LLM output directly** - it's non-deterministic even with temperature=0.3.

**Test Strategy:**
1. **Unit tests:** Mock OpenAI client, return fixtures, test parsing logic
2. **Integration tests:** Use recorded responses from real LLM calls (fixtures)
3. **Manual validation:** Test with 10 sample conversations, manually verify decisions
4. **Fallback tests:** Test heuristics directly (no LLM involved) - these ARE deterministic

**Why this approach:**
- LLMs can return slightly different outputs even with same input + low temperature
- Testing "direct mention always returns shouldNotify=true" will be flaky
- Instead: Test that our **parsing** of LLM responses works correctly
- Use **fallback heuristics** as the deterministic test surface

**Example:**
```typescript
// âŒ BAD TEST (flaky)
it('should notify on direct mention', async () => {
  const result = await analyzeForNotification({ conversationId: 'conv123', userId: 'alice' });
  expect(result.shouldNotify).toBe(true); // Fails sometimes!
});

// âœ… GOOD TEST (deterministic)
it('should parse LLM response correctly', async () => {
  mockOpenAI.returnFixture('mention-response.json'); // Recorded response
  const result = await analyzeForNotification({ conversationId: 'conv123', userId: 'alice' });
  expect(result.shouldNotify).toBe(true);
  expect(result.priority).toBe('high');
});

// âœ… GOOD TEST (fallback heuristics)
it('fallback heuristics detect mention', () => {
  const messages = [{ text: '@alice can you help?', senderId: 'bob' }];
  const decision = fallbackNotificationDecision(messages, 'alice', preferences);
  expect(decision.shouldNotify).toBe(true);
});
```

### Testing Standards

**Test File Location:**
- Unit tests: `functions/test/analyzeForNotification.test.ts`
- Integration tests: `functions/test/integration/notification-analysis.test.ts`
- Fixtures: `functions/test/fixtures/llm-responses/`

**Testing Frameworks:**
- Jest for Cloud Functions unit tests
- Firebase Emulator Suite for integration tests
- Mock OpenAI responses for unit tests (use recorded responses from `fixtures/`)

**Test Coverage Requirements:**
- analyzeForNotification function: 80%+ coverage
- Fallback heuristics: 90%+ coverage (critical path)

**Testing Patterns:**
```typescript
// functions/test/analyzeForNotification.test.ts
import { analyzeForNotification } from '../src/analyzeForNotification';
import { mockOpenAI } from './mocks/openai';

describe('analyzeForNotification', () => {
  beforeEach(() => {
    mockOpenAI.resetHistory();
  });

  it('should notify on direct mention', async () => {
    // Setup test conversation with @mention
    const result = await analyzeForNotification({
      conversationId: 'conv123',
      userId: 'alice'
    });

    expect(result.shouldNotify).toBe(true);
    expect(result.priority).toBe('high');
  });

  it('should not notify on general chat', async () => {
    // Setup test conversation with social messages
    const result = await analyzeForNotification({
      conversationId: 'conv123',
      userId: 'alice'
    });

    expect(result.shouldNotify).toBe(false);
  });

  it('should fallback on AI error', async () => {
    // Mock OpenAI to throw rate limit error
    mockOpenAI.throwError(429);

    const result = await analyzeForNotification({
      conversationId: 'conv123',
      userId: 'alice'
    });

    // Should use fallback heuristics
    expect(result.reason).toContain('fallback');
  });
});
```

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-23 | 1.0 | Initial story creation | Sarah (PO) |

---

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

---

## QA Results

_To be populated by QA agent_
