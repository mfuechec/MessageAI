# Story 6.2: RAG System for Full User Context

## Status

**Draft**

---

## Story

**As a** developer,
**I want** to build a RAG (Retrieval-Augmented Generation) pipeline that provides AI with user's full conversation context,
**so that** notification decisions consider user's entire activity, not just one conversation.

---

## Acceptance Criteria

**Conversation Indexing:**
1. Cloud Function: `indexConversationForRAG` called on-demand during analysis (NOT triggered automatically on new messages)
2. Extracts message embeddings in batches (up to 30 messages at once) using OpenAI Ada (text-embedding-ada-002)
3. Check if messages already embedded before calling API (avoid duplicate embedding cost)
4. Stores embeddings in Firestore `message_embeddings` collection
5. Schema: `{ messageId, conversationId, embedding: [1536 floats], timestamp, participantIds }`
6. **Lazy Embedding Strategy:** Only embed messages when `analyzeForNotification` is called. Reuse embeddings if they exist and are < 7 days old.
7. Composite Firestore index: `conversationId + timestamp` (for fast queries)

**User Context Retrieval:**
8. Cloud Function helper: `getUserRecentContext(userId: String, limit: Int)`
9. Retrieves user's last 100 messages across all conversations (past 7 days)
10. Includes: messages user sent, messages in conversations user participates in, unread counts per conversation
11. Formats context for LLM: conversation summaries, user's participation level, recent topics

**Semantic Search:**
12. Cloud Function helper: `findRelevantMessages(userId: String, query: String, topK: Int)`
13. Converts query to embedding, performs cosine similarity search
14. Returns top K most relevant messages for context window
15. Used to find: past mentions of user, similar urgent requests, decision patterns

**User Preference Loading:**
16. Cloud Function loads user preferences from `users/{userId}/ai_notification_preferences` (schema defined in Story 6.4)
17. Preferences include: enabled, pauseThresholdSeconds, priorityKeywords, activeHours, maxAnalysesPerHour
18. Preferences loaded into AI context for personalized decisions

**Performance:**
19. Embedding generation: < 2 seconds per message batch (only for new/stale embeddings)
20. Context retrieval: < 1 second for 100 messages
21. Semantic search: < 1 second for top 10 results
22. Total RAG overhead: < 3 seconds added to notification analysis

**Testing:**
23. Unit test: Embedding generation produces consistent results
24. Unit test: Embedding cache reuse works (doesn't re-embed existing messages)
25. Integration test: Context retrieval returns correct user messages
26. Integration test: Semantic search finds relevant past messages
27. Performance test: RAG pipeline completes within time limits

---

## Tasks / Subtasks

- [ ] Set up OpenAI API integration in Cloud Functions (AC: 2)
  - [ ] Install OpenAI SDK: `npm install openai` in `functions/` directory
  - [ ] Configure OpenAI API key in Firebase environment config
  - [ ] Create OpenAI client wrapper in `functions/src/services/openai-service.ts`
  - [ ] Add error handling for API failures and rate limits

- [ ] Create indexConversationForRAG Cloud Function (AC: 1-7)
  - [ ] Create `functions/src/helpers/indexConversationForRAG.ts` as helper function (NOT Firestore trigger)
  - [ ] Accept parameters: conversationId, messageIds (array of messages to embed)
  - [ ] Check Firestore for existing embeddings (WHERE messageId IN messageIds AND timestamp > 7 days ago)
  - [ ] Filter to only messages that need embedding (missing or stale)
  - [ ] Extract message text and metadata for filtered messages
  - [ ] Call OpenAI embedding API in batch: `openai.embeddings.create({ model: "text-embedding-ada-002", input: [messageText1, messageText2, ...] })`
  - [ ] Store embeddings in Firestore `message_embeddings` collection
  - [ ] Return: { embeddedCount, reusedCount, totalTime }
  - [ ] Implement retry logic for failed embeddings

- [ ] Create Firestore indexes (AC: 5)
  - [ ] Update `firestore.indexes.json` with composite index: `message_embeddings` on `conversationId + timestamp`
  - [ ] Deploy indexes: `firebase deploy --only firestore:indexes`

- [ ] Implement getUserRecentContext helper (AC: 6-9)
  - [ ] Create `functions/src/helpers/user-context.ts`
  - [ ] Query Firestore for user's messages in past 7 days
  - [ ] Query conversations where user is participant
  - [ ] Aggregate unread counts per conversation
  - [ ] Format output: `{ userId, recentMessages: [...], conversations: [...], unreadCounts: {...} }`
  - [ ] Add caching: Store context in Firestore with 10-minute TTL

- [ ] Implement findRelevantMessages semantic search (AC: 10-13)
  - [ ] Create `functions/src/helpers/semantic-search.ts`
  - [ ] Convert query text to embedding using OpenAI
  - [ ] Load user's message embeddings from Firestore
  - [ ] Calculate cosine similarity: `dot(queryEmbedding, messageEmbedding) / (norm(queryEmbedding) * norm(messageEmbedding))`
  - [ ] Return top K messages sorted by similarity score
  - [ ] Add filtering: Only search user's conversations

- [ ] Create helper to load user preferences (AC: 16-18)
  - [ ] Define TypeScript interface in `functions/src/types/NotificationPreferences.ts` (schema owned by Story 6.4)
  - [ ] Implement `getNotificationPreferences(userId: String)` helper
  - [ ] Load preferences from Firestore: `users/{userId}/ai_notification_preferences`
  - [ ] Return default values if preferences don't exist (user hasn't opted in yet)

- [ ] Add preference loading to notification analysis (AC: 17)
  - [ ] Update `analyzeForNotification` Cloud Function (Story 6.3 integration)
  - [ ] Load user preferences from Firestore
  - [ ] Include preferences in LLM context: priority keywords, active hours, analysis rate

- [ ] Write unit tests (AC: 22)
  - [ ] Test: Embedding generation returns 1536-dimension array
  - [ ] Test: Same input text produces same embedding (deterministic)
  - [ ] Test: Context retrieval returns correct number of messages
  - [ ] Test: Semantic search ranks similar messages higher

- [ ] Write integration tests (AC: 23-24)
  - [ ] Test: indexConversationForRAG triggered on new message
  - [ ] Test: Embedding stored in Firestore with correct schema
  - [ ] Test: getUserRecentContext returns user's messages only
  - [ ] Test: findRelevantMessages finds semantically similar messages
  - [ ] Test: User preferences loaded correctly

- [ ] Performance testing (AC: 18-21)
  - [ ] Measure embedding generation time (target: < 2 seconds)
  - [ ] Measure context retrieval time (target: < 1 second)
  - [ ] Measure semantic search time (target: < 1 second)
  - [ ] Measure total RAG overhead (target: < 3 seconds)
  - [ ] Optimize if performance targets not met

---

## Dev Notes

### Architecture Context

**Cloud Functions Infrastructure:**
This story builds the RAG (Retrieval-Augmented Generation) pipeline as a set of Cloud Functions and Firestore collections. The RAG system provides rich context to the AI notification analysis function (Story 6.3).

**Data Flow:**
```
analyzeForNotification (Story 6.3) called
  ↓
indexConversationForRAG (on-demand) → Check existing embeddings → OpenAI Embedding API (only new/stale) → Firestore (message_embeddings)
  ↓
getUserRecentContext → Firestore queries → Context object
  ↓
findRelevantMessages → Semantic search → Top K messages
  ↓
Load preferences → Firestore query → User preferences (from Story 6.4)
```

### Relevant Source Tree

**New Files:**
- `functions/src/indexConversationForRAG.ts` - Cloud Function for message embedding
- `functions/src/helpers/user-context.ts` - getUserRecentContext implementation
- `functions/src/helpers/semantic-search.ts` - findRelevantMessages implementation
- `functions/src/services/openai-service.ts` - OpenAI client wrapper
- `functions/src/types/NotificationPreferences.ts` - TypeScript types
- `firestore.indexes.json` - Add composite index for message_embeddings

**Modified Files:**
- `functions/package.json` - Add `openai` dependency
- `functions/src/index.ts` - Export new Cloud Functions

### Key Technical Decisions

**1. Embedding Model Selection:**
Use OpenAI `text-embedding-ada-002` (1536 dimensions):
- Cost: $0.0001 per 1K tokens (very cheap)
- Quality: High semantic understanding
- Consistency: Deterministic for same input
- Speed: ~500ms per embedding

**2. Firestore Schema for Embeddings:**
```typescript
// message_embeddings/{messageId}
{
  messageId: string,
  conversationId: string,
  embedding: number[], // [1536 floats]
  timestamp: Timestamp,
  participantIds: string[],
  messageText: string // Store for debugging, optional
}
```

**3. Cosine Similarity Implementation:**
```typescript
function cosineSimilarity(a: number[], b: number[]): number {
  const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0);
  const normA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
  const normB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
  return dotProduct / (normA * normB);
}
```

**4. Context Retrieval Strategy:**
Retrieve two types of context:
- **Recent Activity:** Last 100 messages across all user's conversations (chronological)
- **Semantic Context:** Top 10 messages semantically similar to current conversation topic

Combined context provides both:
- Temporal awareness (what happened recently)
- Semantic awareness (related past discussions)

**5. Caching Strategy:**
Cache user context with short TTL (10 minutes):
```typescript
// user_context_cache/{userId}
{
  userId: string,
  context: object, // Full context object
  generatedAt: Timestamp,
  expiresAt: Timestamp // +10 minutes
}
```

Why 10 minutes? Balance freshness vs cost. Context changes slowly, caching reduces Firestore reads and OpenAI API calls.

**6. Firestore Indexes:**
```json
// firestore.indexes.json
{
  "indexes": [
    {
      "collectionGroup": "message_embeddings",
      "queryScope": "COLLECTION",
      "fields": [
        { "fieldPath": "conversationId", "order": "ASCENDING" },
        { "fieldPath": "timestamp", "order": "DESCENDING" }
      ]
    },
    {
      "collectionGroup": "message_embeddings",
      "queryScope": "COLLECTION",
      "fields": [
        { "fieldPath": "participantIds", "arrayConfig": "CONTAINS" },
        { "fieldPath": "timestamp", "order": "DESCENDING" }
      ]
    }
  ]
}
```

### OpenAI Integration

**Prerequisites - Complete Story 6.0 first for environment setup**

**Installation:**
```bash
cd functions
npm install openai
```

**Configuration:**
```bash
# Set OpenAI API key in Firebase
firebase functions:config:set openai.api_key="sk-..." --project=messageai-dev-1f2ec

# Deploy config
firebase deploy --only functions
```

**For local testing with emulator:**
Create `.runtimeconfig.json` in `functions/` directory:
```json
{
  "openai": {
    "api_key": "sk-your-key-here"
  }
}
```
*Add `.runtimeconfig.json` to `.gitignore`*

**Usage:**
```typescript
// functions/src/services/openai-service.ts
import OpenAI from 'openai';
import * as functions from 'firebase-functions';

const openai = new OpenAI({
  apiKey: functions.config().openai.api_key
});

export async function generateEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: "text-embedding-ada-002",
    input: text
  });
  return response.data[0].embedding;
}
```

### User Context Format

```typescript
interface UserContext {
  userId: string;
  recentMessages: {
    messageId: string;
    conversationId: string;
    text: string;
    timestamp: Date;
    senderId: string;
  }[];
  conversations: {
    conversationId: string;
    participantIds: string[];
    isGroup: boolean;
    lastMessageTimestamp: Date;
    unreadCount: number;
  }[];
  preferences: {
    enabled: boolean;
    pauseThresholdSeconds: number;
    activeHours: { start: string; end: string; timezone: string };
    priorityKeywords: string[];
    maxAnalysesPerHour: number;
  };
  semanticContext?: {
    messageId: string;
    text: string;
    similarity: number;
  }[];
}
```

### Notification Preferences Schema

```typescript
// users/{userId}/ai_notification_preferences
interface NotificationPreferences {
  enabled: boolean;
  pauseThresholdSeconds: number; // Default: 120
  activeConversationThreshold: number; // Default: 20
  quietHoursStart: string; // "22:00"
  quietHoursEnd: string; // "08:00"
  timezone: string; // "America/Los_Angeles"
  priorityKeywords: string[]; // ["urgent", "ASAP", "production down", "blocker"]
  maxAnalysesPerHour: number; // Default: 10
  fallbackStrategy: "simple_rules" | "notify_all" | "suppress_all";
  createdAt: Timestamp;
  updatedAt: Timestamp;
}
```

### Cost Estimation

**Per Message Indexed:**
- OpenAI embedding: ~100 tokens = $0.00001
- Firestore write: $0.00018
- **Total: ~$0.0002 per message**

**Per Notification Analysis (with RAG):**
- Context retrieval (100 messages): $0 (Firestore reads, ~$0.00036)
- Semantic search embedding: $0.00001
- Cosine similarity calculation: $0 (local compute)
- **Total RAG overhead: ~$0.0004**

**Monthly Cost (1000 active users, 100 messages/user/day):**
- Indexing: 1000 users * 100 msgs * 30 days * $0.0002 = $600/month
- Context retrieval: 1000 users * 10 analyses/day * 30 days * $0.0004 = $120/month
- **Total: ~$720/month for RAG infrastructure**

Note: This is a demo, costs not a concern per requirements. Production would need aggressive caching and batch optimization.

### Testing Standards

**Test File Location:**
- Unit tests: `functions/test/helpers/user-context.test.ts`, `functions/test/helpers/semantic-search.test.ts`
- Integration tests: `functions/test/integration/rag-pipeline.test.ts`

**Testing Frameworks:**
- Jest for Cloud Functions unit tests
- Firebase Emulator Suite for integration tests
- Use mock OpenAI responses for unit tests (avoid API costs)

**Test Coverage Requirements:**
- Cloud Functions: 70%+ coverage
- Critical paths (embedding, context retrieval): 90%+ coverage

**Testing Patterns:**
```typescript
// functions/test/helpers/semantic-search.test.ts
import { findRelevantMessages } from '../../src/helpers/semantic-search';
import { mockOpenAI } from '../mocks/openai';

describe('findRelevantMessages', () => {
  beforeEach(() => {
    mockOpenAI.resetHistory();
  });

  it('should return top K messages by similarity', async () => {
    const userId = 'user123';
    const query = 'urgent production issue';
    const topK = 5;

    const results = await findRelevantMessages(userId, query, topK);

    expect(results).toHaveLength(5);
    expect(results[0].similarity).toBeGreaterThan(results[4].similarity);
  });

  it('should only search user\'s conversations', async () => {
    // Test filtering to user's messages only
  });
});
```

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-23 | 1.0 | Initial story creation | Sarah (PO) |

---

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

---

## QA Results

_To be populated by QA agent_
